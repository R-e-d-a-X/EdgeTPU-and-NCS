{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-03 23:38:23.323069: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from conversion_tf import GEMMDecisionTreeImplKeras, GEMMDecisionTreeImpl\n",
    "from hummingbird.ml import convert\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "import numpy as np\n",
    "from openvino.runtime import Core\n",
    "import time\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-31 16:33:40.984921: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-31 16:33:40.986359: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "def representative_dataset():\n",
    "    for _ in range(100):\n",
    "      data = np.random.uniform(low=0., high=8., size=(1,8))\n",
    "      yield [data.astype(np.float32)]\n",
    " \n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=1, max_depth=1)\n",
    "X, y = make_classification(n_samples=1300, n_features=4,\n",
    "                           n_informative=4, n_redundant=0,\n",
    "                           random_state=0, shuffle=True,\n",
    "                           n_classes=4)\n",
    "\n",
    "x_train, y_train = X[:1000], y[:1000]\n",
    "x_test, y_test = X[1000:], y[1000:]\n",
    "\n",
    "forest.fit(x_train, y_train)\n",
    "\n",
    "X = tf.constant([1, 2, 3, 4, 5, 6, 7, 8], shape=[1, 8], dtype=tf.int32)\n",
    "X_float = tf.constant([1., 2., 3., 4., 5., 6., 7., 8.], shape=[1, 8])\n",
    "X_8 = tf.constant([1, 2, 3, 4, 5, 6, 7, 8], shape=[1, 8], dtype=tf.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gemm = GEMMDecisionTreeImpl(forest)\n",
    "keras_model_gemm = GEMMDecisionTreeImplKeras(forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[1.,2.,3.,4.]], dtype=np.float32)\n",
    "x_8 = np.array([[1, 2]], dtype=np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = (14) * np.random.random_sample((32,8)) - 7.1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NORMAL GEMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_gemm = time.perf_counter()\n",
    "res_gemm = model_gemm(x)\n",
    "time_gemm = (time.perf_counter() - start_gemm) * 1000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KERAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_keras = time.perf_counter()\n",
    "res_keras = keras_model_gemm(x)\n",
    "time_keras = (time.perf_counter() - start_keras) * 1000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPENVINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Dims and format are inconsistent.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m output_layer \u001b[39m=\u001b[39m compiled_model\u001b[39m.\u001b[39moutput(\u001b[39m0\u001b[39m)\n\u001b[1;32m     16\u001b[0m start_vino \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mperf_counter()\n\u001b[0;32m---> 17\u001b[0m res_vino \u001b[39m=\u001b[39m compiled_model([np\u001b[39m.\u001b[39;49marray([\u001b[39m1\u001b[39;49m,\u001b[39m2\u001b[39;49m,\u001b[39m3\u001b[39;49m,\u001b[39m4\u001b[39;49m,\u001b[39m5\u001b[39;49m,\u001b[39m6\u001b[39;49m,\u001b[39m7\u001b[39;49m,\u001b[39m8\u001b[39;49m])])[output_layer]\n\u001b[1;32m     18\u001b[0m time_vino \u001b[39m=\u001b[39m (time\u001b[39m.\u001b[39mperf_counter() \u001b[39m-\u001b[39m start_vino) \u001b[39m*\u001b[39m \u001b[39m1000\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/BA2/lib/python3.8/site-packages/openvino/runtime/ie_api.py:273\u001b[0m, in \u001b[0;36mCompiledModel.__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, inputs: Optional[Union[\u001b[39mdict\u001b[39m, \u001b[39mlist\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mdict\u001b[39m:\n\u001b[1;32m    269\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Callable infer wrapper for CompiledModel.\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \n\u001b[1;32m    271\u001b[0m \u001b[39m    Take a look at `infer_new_request` for reference.\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 273\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minfer_new_request(inputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/BA2/lib/python3.8/site-packages/openvino/runtime/ie_api.py:266\u001b[0m, in \u001b[0;36mCompiledModel.infer_new_request\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Infers specified input(s) in synchronous mode.\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \n\u001b[1;32m    238\u001b[0m \u001b[39mBlocks all methods of CompiledModel while request is running.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[39m:rtype: Dict[openvino.runtime.ConstOutput, numpy.array]\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m# It returns wrapped python InferReqeust and then call upon\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[39m# overloaded functions of InferRequest class\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcreate_infer_request()\u001b[39m.\u001b[39;49minfer(inputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/BA2/lib/python3.8/site-packages/openvino/runtime/ie_api.py:152\u001b[0m, in \u001b[0;36mInferRequest.infer\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[39m# If inputs are list or tuple, enumarate inputs and save them as dictionary.\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[39m# It is an extension of above branch with dict inputs.\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(inputs, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[0;32m--> 152\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39minfer(normalize_inputs(\u001b[39mself\u001b[39;49m, {index: \u001b[39minput\u001b[39;49m \u001b[39mfor\u001b[39;49;00m index, \u001b[39minput\u001b[39;49m \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(inputs)}))\n\u001b[1;32m    153\u001b[0m \u001b[39m# If inputs are Tensor, call infer method directly.\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(inputs, Tensor):\n",
      "File \u001b[0;32m~/anaconda3/envs/BA2/lib/python3.8/site-packages/openvino/runtime/ie_api.py:100\u001b[0m, in \u001b[0;36mnormalize_inputs\u001b[0;34m(request, inputs)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[39m# Copy numpy arrays to already allocated Tensors.\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, (np\u001b[39m.\u001b[39mndarray, np\u001b[39m.\u001b[39mnumber, \u001b[39mint\u001b[39m, \u001b[39mfloat\u001b[39m)):\n\u001b[0;32m--> 100\u001b[0m     update_tensor(value, request, key)\n\u001b[1;32m    101\u001b[0m \u001b[39m# If value is of Tensor type, put it into temporary dictionary.\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, Tensor):\n",
      "File \u001b[0;32m~/anaconda3/envs/BA2/lib/python3.8/functools.py:875\u001b[0m, in \u001b[0;36msingledispatch.<locals>.wrapper\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    871\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[1;32m    872\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mfuncname\u001b[39m}\u001b[39;00m\u001b[39m requires at least \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    873\u001b[0m                     \u001b[39m'\u001b[39m\u001b[39m1 positional argument\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 875\u001b[0m \u001b[39mreturn\u001b[39;00m dispatch(args[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49m\u001b[39m__class__\u001b[39;49m)(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n",
      "File \u001b[0;32m~/anaconda3/envs/BA2/lib/python3.8/site-packages/openvino/runtime/ie_api.py:65\u001b[0m, in \u001b[0;36m_\u001b[0;34m(inputs, request, key)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39m# Update shape if there is a mismatch\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mif\u001b[39;00m tensor\u001b[39m.\u001b[39mshape \u001b[39m!=\u001b[39m inputs\u001b[39m.\u001b[39mshape:\n\u001b[0;32m---> 65\u001b[0m     tensor\u001b[39m.\u001b[39;49mshape \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mshape\n\u001b[1;32m     66\u001b[0m \u001b[39m# When copying, type should be up/down-casted automatically.\u001b[39;00m\n\u001b[1;32m     67\u001b[0m tensor\u001b[39m.\u001b[39mdata[:] \u001b[39m=\u001b[39m inputs[:]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Dims and format are inconsistent."
     ]
    }
   ],
   "source": [
    "model_xml = f\"../../benchmarks/ncs/TT_scaling_trees_1.xml\"\n",
    "\n",
    "# Load model\n",
    "ie = Core()\n",
    "model = ie.read_model(model=model_xml)\n",
    "\n",
    "# Neural Compute Stick\n",
    "# compiled_model = ie.compile_model(model=model, device_name=\"MYRIAD\")\n",
    "compiled_model = ie.compile_model(model=model, device_name=\"CPU\")\n",
    "\n",
    "del model\n",
    "\n",
    "input_layer = compiled_model.input(0)\n",
    "output_layer = compiled_model.output(0)\n",
    "\n",
    "start_vino = time.perf_counter()\n",
    "res_vino = compiled_model([np.array([1,2,3,4,5,6,7,8])])[output_layer]\n",
    "time_vino = (time.perf_counter() - start_vino) * 1000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HUMMINGBIRD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = convert(forest, 'torch', extra_config={\"tree_implementation\":\"gemm\"})\n",
    "hb_model = container.model._operators[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_hb = time.perf_counter()\n",
    "res_hb = container.predict(x)\n",
    "time_hb = (time.perf_counter() - start_hb) * 1000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF LITE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter('../../saved_models/edgetpu/random_forest/gemm/float32/final_eval/benchmarked/model_1_4_2_1.tflite')\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "output = interpreter.get_output_details()[0]  \n",
    "input = interpreter.get_input_details()[0]  \n",
    "\n",
    "interpreter.set_tensor(input['index'], x_8)\n",
    "\n",
    "start_lite = time.perf_counter()\n",
    "interpreter.invoke()\n",
    "time_lite = (time.perf_counter() - start_lite) * 1000\n",
    "pred_lite = interpreter.get_tensor(output['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.22851562],\n",
       "        [0.04525757],\n",
       "        [0.23754883],\n",
       "        [0.48876953]]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_vino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0]), tensor([[0.5129, 0.1392, 0.0825, 0.2655]], grad_fn=<TBackward0>))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hb_model.forward(torch.tensor(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 4), dtype=float32, numpy=array([[0.5128866 , 0.13917525, 0.08247422, 0.26546392]], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_keras"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL_GEMM: \t 2.10ms\n",
      "MODEL_KERAS:\t 2.79ms\n",
      "OPENVINO:   \t 0.24ms\n",
      "HUMMINGBIRD:\t 1.19ms\n",
      "TFLITE:     \t 0.06ms\n"
     ]
    }
   ],
   "source": [
    "print(f'MODEL_GEMM: \\t{time_gemm : .2f}ms')\n",
    "print(f'MODEL_KERAS:\\t{time_keras : .2f}ms')\n",
    "print(f'OPENVINO:   \\t{time_vino : .2f}ms')\n",
    "print(f'HUMMINGBIRD:\\t{time_hb : .2f}ms')\n",
    "print(f'TFLITE:     \\t{time_lite : .2f}ms')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BA2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d5b96343b31861d80b9d3de750e5a5e11a0f674dbcb91b0b966fee3e7b7f3d0b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
