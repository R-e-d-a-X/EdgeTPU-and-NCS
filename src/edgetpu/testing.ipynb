{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-07 00:33:24.076309: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "import tensorflow as tf\n",
    "from conversion_tf import GEMMDecisionTreeImpl, TreeTraversalDecisionTreeImpl, PerfectTreeTraversalDecisionTreeImpl\n",
    "import numpy as np\n",
    "from hummingbird.ml import convert\n",
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Important__ <br>\n",
    "__DONT__ run all off the examples, because this machine will run out of memory <br>\n",
    "__ALSO__ the __Ptt__ example uses a small RF because of memory <br>\n",
    "__GENERALLY__ all 3 methods work correctly and are tflite convertable <br>\n",
    "__BUT__ they arent __Edge TPU__ compatible yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-07 00:33:34.072296: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-07 00:33:34.073122: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "def representative_dataset():\n",
    "    for _ in range(100):\n",
    "      data = np.random.uniform(low=0., high=8., size=(1,8))\n",
    "      yield [data.astype(np.float32)]\n",
    " \n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=105)\n",
    "forest2 = RandomForestClassifier(n_estimators=25)\n",
    "X, y = make_classification(n_samples=1300, n_features=8,\n",
    "                           n_informative=4, n_redundant=1,\n",
    "                           random_state=0, shuffle=True,\n",
    "                           n_classes=4)\n",
    "\n",
    "x_train, y_train = X[:1000], y[:1000]\n",
    "x_test, y_test = X[1000:], y[1000:]\n",
    "\n",
    "forest.fit(x_train, y_train)\n",
    "forest2.fit(x_train, y_train)\n",
    "\n",
    "X = tf.constant([1, 2, 3, 4, 5, 6, 7, 8], shape=[1, 8], dtype=tf.int8)\n",
    "X_float = tf.constant([1., 2., 3., 4., 5., 6., 7., 8.], shape=[1, 8])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TTDecisionTreeTraversalImpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_model = convert(forest, 'torch', extra_config={\"tree_implementation\":\"tree_trav\"})\n",
    "\n",
    "y_mod_pred_tt, y_mod_tt = conv_model.model._operators[0].forward((torch.tensor([[1., 2., 3., 4., 5., 6., 7., 8.]])))\n",
    "\n",
    "model_tt = TreeTraversalDecisionTreeImpl(forest)\n",
    "\n",
    "y_pred_tt, y_tt = model_tt(X_float)\n",
    "\n",
    "concrete_func = model_tt.__call__.get_concrete_function()\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func], model_tt)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8  \n",
    "converter.inference_output_type = tf.int8  \n",
    "tflite_model_tt = converter.convert()\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_content=tflite_model_tt)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "output = interpreter.get_output_details()[0]  \n",
    "input = interpreter.get_input_details()[0]  \n",
    "\n",
    "interpreter.set_tensor(input['index'], X)\n",
    "interpreter.invoke()\n",
    "y_lite_tt = interpreter.get_tensor(output['index'])\n",
    "\n",
    "y_pred_lite_tt = np.argmax(y_lite_tt, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'############################################ TT ###############################################################\\n')\n",
    "print(f'Prediction original    : {y_mod_pred_tt.detach().numpy()[0]} \\t Class Confidence: {y_mod_tt.detach().numpy()[0]}')\n",
    "print(f'Prediction tf module   : {y_pred_tt.numpy()[0]} \\t Class Confidence: {y_tt.numpy()[0]}')\n",
    "print(f'Prediction tflite quant: {y_pred_lite_tt[0]} \\t Class Confidence: {y_lite_tt[0]}')\n",
    "print(f'\\n###############################################################################################################\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PerfectDecisionTreeTraversalImpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmphad3nn23/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-07 00:34:03.608154: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n",
      "2023-02-07 00:34:03.608175: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n",
      "2023-02-07 00:34:03.608636: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmphad3nn23\n",
      "2023-02-07 00:34:03.915249: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-02-07 00:34:03.915284: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/tmphad3nn23\n",
      "2023-02-07 00:34:03.915953: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "2023-02-07 00:34:04.947629: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n",
      "2023-02-07 00:34:04.966687: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle.\n",
      "2023-02-07 00:34:06.778115: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: /tmp/tmphad3nn23\n",
      "2023-02-07 00:34:08.460721: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 4852091 microseconds.\n",
      "2023-02-07 00:34:13.858620: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-02-07 00:34:13.880487: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "2023-02-07 00:34:14.332372: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1989] Estimated count of arithmetic ops: 925  ops, equivalently 462  MACs\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated count of arithmetic ops: 925  ops, equivalently 462  MACs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fully_quantize: 0, inference_type: 6, input_inference_type: INT8, output_inference_type: INT8\n",
      "2023-02-07 00:34:24.052463: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1989] Estimated count of arithmetic ops: 925  ops, equivalently 462  MACs\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated count of arithmetic ops: 925  ops, equivalently 462  MACs\n"
     ]
    }
   ],
   "source": [
    "conv_model = convert(forest2, 'torch', extra_config={\"tree_implementation\":\"perf_tree_trav\"})\n",
    "\n",
    "y_mod_pred_ptt, y_mod_ptt = conv_model.model._operators[0].forward((torch.tensor([[1., 2., 3., 4., 5., 6., 7., 8.]])))\n",
    "\n",
    "model_ptt = PerfectTreeTraversalDecisionTreeImpl(forest2)\n",
    "\n",
    "y_pred_ptt, y_ptt = model_ptt(X_float)\n",
    "\n",
    "concrete_func = model_ptt.__call__.get_concrete_function()\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func], model_ptt)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8  \n",
    "converter.inference_output_type = tf.int8  \n",
    "tflite_model_ptt = converter.convert()\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_content=tflite_model_ptt)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "output = interpreter.get_output_details()[0]  \n",
    "input = interpreter.get_input_details()[0]  \n",
    "\n",
    "interpreter.set_tensor(input['index'], X)\n",
    "interpreter.invoke()\n",
    "y_lite_ptt = interpreter.get_tensor(output['index'])\n",
    "\n",
    "y_pred_lite_ptt = np.argmax(y_lite_ptt, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############################################ PTT ##############################################################\n",
      "\n",
      "Prediction original    : 1 \t Class Confidence: [0.19999999 0.43999997 0.         0.35999995]\n",
      "Prediction tf module   : 1 \t Class Confidence: [0.19999999 0.43999997 0.         0.36      ]\n",
      "Prediction tflite quant: 0 \t Class Confidence: [ 124  118 -128  117]\n",
      "\n",
      "###############################################################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'\\n############################################ PTT ##############################################################\\n')\n",
    "print(f'Prediction original    : {y_mod_pred_ptt.detach().numpy()[0]} \\t Class Confidence: {y_mod_ptt.detach().numpy()[0]}')\n",
    "print(f'Prediction tf module   : {y_pred_ptt.numpy()[0]} \\t Class Confidence: {y_ptt.numpy()[0]}')\n",
    "print(f'Prediction tflite quant: {y_pred_lite_ptt[0]} \\t Class Confidence: {y_lite_ptt[0]}')\n",
    "print(f'\\n###############################################################################################################\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GEMMDecisionTreeImpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gemm = GEMMDecisionTreeImpl(forest)\n",
    "\n",
    "conv_model = convert(forest, 'torch', extra_config={\"tree_implementation\":\"gemm\"})\n",
    "\n",
    "y_mod_pred_gemm, y_mod_gemm = conv_model.model._operators[0].forward((torch.tensor([[1., 2., 3., 4., 5., 6., 7., 8.]])))\n",
    "\n",
    "y_pred_gemm, y_gemm = model_gemm(X_float)\n",
    "\n",
    "concrete_func = model_gemm.__call__.get_concrete_function()\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func], model_gemm)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8  \n",
    "converter.inference_output_type = tf.int8  \n",
    "tflite_model_gemm = converter.convert()\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_content=tflite_model_gemm)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "output = interpreter.get_output_details()[0]  \n",
    "input = interpreter.get_input_details()[0]  \n",
    "\n",
    "\n",
    "interpreter.set_tensor(input['index'], X)\n",
    "interpreter.invoke()\n",
    "y_lite_gemm = interpreter.get_tensor(output['index'])\n",
    "\n",
    "y_pred_lite_gemm = np.argmax(y_lite_gemm, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'\\n############################################ GEMM #############################################################\\n')\n",
    "print(f'Prediction original    : {y_mod_pred_gemm.detach().numpy()[0]} \\t Class Confidence: {y_mod_gemm.detach().numpy()[0]}')\n",
    "print(f'Prediction tf module   : {y_pred_gemm.numpy()[0]} \\t Class Confidence: {y_gemm.numpy()[0]}')\n",
    "print(f'Prediction tflite quant: {y_pred_lite_gemm[0]} \\t Class Confidence: {y_lite_gemm[0]}')\n",
    "print(f'\\n###############################################################################################################\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BA2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d5b96343b31861d80b9d3de750e5a5e11a0f674dbcb91b0b966fee3e7b7f3d0b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
